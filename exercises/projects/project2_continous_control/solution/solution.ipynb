{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program. We also demonstrate a possible solution for the second project.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).\n",
    "\n",
    "We will use here the headless version since the project is set up in a dockerized environment to execute it on remote computing (possible GPU enabled)\n",
    "resources as well. \n",
    "\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='../Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment note: train_mode is True since we would like to train our agent here\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the agent with DDPG\n",
    "\n",
    "During the assigment we train the agent with `Deep Deterministic Policy Gradient` algorithm. You can find an outstanding description about it \n",
    "[here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html). The underlying deep(? - maybe rather wide than deep) neural networks for the actor and the critic are defined in [model.py](./model.py). The agent itself in the [ddpg_agent.py](./ddpg_agent.py). These implementations are based on the examples provided for the course for the actor-critic methods chapter [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal) or [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum). \n",
    "\n",
    "I choose to solve the First Version as: the task is episodic, and in order to solve the environment, the agent must get an average score of +30 over 100 consecutive episodes.\n",
    "\n",
    "Next we define a function to execute the training itself, which we will use for later experiments with the hyperparams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=10, max_t=1000, print_every=5, agent=Agent(33,4,42)):\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes+1).start()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_all = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "        state = env_info.vector_observations[0]  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)\n",
    "        mean_score = 0\n",
    "        agent.reset()\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            scores += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        score = np.mean(scores)\n",
    "        scores_deque.append(score)\n",
    "        mean_score = np.mean(scores_deque)\n",
    "        scores_all.append(score)\n",
    "\n",
    "        if i_episode % print_every == 0 or (len(scores_deque) == 100 and np.mean(scores_deque) >= 30):\n",
    "            timer.update(i_episode+1)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMin Score: {:.2f}\\tMax Score: {:.2f}'.format(i_episode, np.mean(scores_deque),\n",
    "                                                                                                    np.min(scores_deque), np.max(scores_deque)))\n",
    "\n",
    "        if len(scores_deque) == 100 and np.mean(scores_deque) >= 30:  \n",
    "            print('Environment solved !')\n",
    "            break\n",
    "\n",
    "    timer.finish()\n",
    "    return scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Baseline for experiments\n",
    "\n",
    "Here we define our baseline for the later experiments, to check the effect of the chosen parameters on the convergence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_default = ddpg(n_episodes=1000, max_t=1000, print_every=50,\n",
    "              agent=Agent(state_size=33,action_size=4,random_seed =42,actor_fc1_size=128, actor_fc2_size=128,\n",
    "                 actor_fc3_size=64, critic_fcs1_size=128, critic_fc2_size=128, critic_fc3_size=64,\n",
    "                 lr_actor=0.0001, lr_critic=0.0001, batch_size=128, buffer_size=1e5, gamma=0.99,\n",
    "                 tau=0.001, weight_decay=0))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(np.arange(1, len(scores_default)+1),np.squeeze(np.vstack(scores_default)), label='default')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Test run scores')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Experiments - learning rate\n",
    "\n",
    "Thus we can see that the convergence is rather slow , we try to increase the learning rate to speed up the training of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lr1 = ddpg(n_episodes=1000, max_t=3000, print_every=50,\n",
    "              agent=Agent(state_size=33,action_size=4,random_seed =42,actor_fc1_size=512, actor_fc2_size=256,\n",
    "                 actor_fc3_size=128, critic_fcs1_size=512, critic_fc2_size=256, critic_fc3_size=128,\n",
    "                 lr_actor=0.0002, lr_critic=0.0002, batch_size=128, buffer_size=1e5, gamma=0.99,\n",
    "                 tau=0.001, weight_decay=0))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(np.arange(1, len(scores_lr2)+1),np.squeeze(np.vstack(scores_lr2)), label='default')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Test run scores')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
