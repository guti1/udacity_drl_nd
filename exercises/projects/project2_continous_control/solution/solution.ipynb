{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program. We also demonstrate a possible solution for the second project.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).\n",
    "\n",
    "We will use here the headless version since the project is set up in a dockerized environment to execute it on remote computing (possible GPU enabled)\n",
    "resources as well. \n",
    "\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "from ddpg_utils import ddpg_train, ddpg_present, plot_experiment\n",
    "\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"../Reacher2_Linux_NoVis/Reacher.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment note: train_mode is True since we would like to train our agent here\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print(\"Number of agents:\", num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the agent with DDPG\n",
    "\n",
    "During the assigment we train the agent with `Deep Deterministic Policy Gradient` algorithm. You can find an outstanding description about it \n",
    "[here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html). The underlying deep(? - maybe rather wide than deep) neural networks for the actor and the critic are defined in [model.py](./model.py). The agent itself in the [ddpg_agent.py](./ddpg_agent.py). These implementations are based on the examples provided for the course for the actor-critic methods chapter [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal) or [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum). \n",
    "\n",
    "I choose to solve the second version with 20 agents as: the task is episodic, and in order to solve the environment, the agents need get an average score of +30 over 100 consecutive episodes.\n",
    "\n",
    "Next we define the hyperparams we would like to use and compare the convergence applying them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params = {\n",
    "    \"ACTOR_HIDDEN_LAYERS\": [400, 300],\n",
    "    \"CRITIC_HIDDEN_LAYERS\": [400, 300],\n",
    "    \"USE_BATCH_NORM\": True,\n",
    "    \"ADD_NOISE\": True,\n",
    "    \"GRAD_CLIP\": False,\n",
    "    \"BUFFER_SIZE\": int(1e5),\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"TAU\": 1e-3,\n",
    "    \"LR_ACTOR\": 1e-4,\n",
    "    \"LR_CRITIC\": 1e-4,\n",
    "    \"WEIGHT_DECAY\": 0,\n",
    "    \"UPDATE_EVERY\": 1,\n",
    "    \"UPDATE_FREQ\": 1,\n",
    "    \"USE_XAVIER\": False,\n",
    "}\n",
    "agent_params1 = {**agent_params,\n",
    "                 \"ACTOR_HIDDEN_LAYERS\": [400, 300, 200, 100],\n",
    "                 \"CRITIC_HIDDEN_LAYERS\": [400, 300, 200, 100],\n",
    "                 }\n",
    "\n",
    "agent_params2 = {**agent_params,\n",
    "                 \"LR_CRITIC\": 1e-3,}\n",
    "agent_params3 = {**agent_params1,\n",
    "                 \"LR_CRITIC\": 1e-3,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACTOR_HIDDEN_LAYERS': [400, 300], 'CRITIC_HIDDEN_LAYERS': [400, 300], 'USE_BATCH_NORM': True, 'ADD_NOISE': True, 'GRAD_CLIP': False, 'BUFFER_SIZE': 100000, 'BATCH_SIZE': 128, 'GAMMA': 0.99, 'TAU': 0.001, 'LR_ACTOR': 0.0001, 'LR_CRITIC': 0.0001, 'WEIGHT_DECAY': 0, 'UPDATE_EVERY': 1, 'UPDATE_FREQ': 1, 'USE_XAVIER': False}\n",
      "Episode 5\tAverage Score: 0.95\tMax Score: 1.56\tStd Score: 0.35\n",
      "Episode 10\tAverage Score: 1.35\tMax Score: 2.63\tStd Score: 0.58\n",
      "Episode 15\tAverage Score: 2.21\tMax Score: 4.79\tStd Score: 1.39\n",
      "Episode 20\tAverage Score: 3.84\tMax Score: 15.24\tStd Score: 3.71\n",
      "Episode 25\tAverage Score: 8.49\tMax Score: 34.57\tStd Score: 10.10\n",
      "Episode 30\tAverage Score: 13.08\tMax Score: 37.17\tStd Score: 13.80\n",
      "Episode 35\tAverage Score: 16.57\tMax Score: 37.89\tStd Score: 15.38\n",
      "Episode 40\tAverage Score: 19.13\tMax Score: 37.89\tStd Score: 15.90\n",
      "Episode 45\tAverage Score: 21.13\tMax Score: 37.89\tStd Score: 16.02\n",
      "Episode 50\tAverage Score: 22.68\tMax Score: 37.89\tStd Score: 15.90\n",
      "Episode 55\tAverage Score: 23.84\tMax Score: 37.89\tStd Score: 15.59\n",
      "Episode 60\tAverage Score: 24.79\tMax Score: 37.89\tStd Score: 15.27\n",
      "Episode 65\tAverage Score: 25.66\tMax Score: 37.89\tStd Score: 14.97\n",
      "Episode 70\tAverage Score: 26.43\tMax Score: 37.89\tStd Score: 14.69\n",
      "Episode 75\tAverage Score: 27.09\tMax Score: 37.89\tStd Score: 14.41\n",
      "Episode 80\tAverage Score: 27.69\tMax Score: 37.89\tStd Score: 14.14\n",
      "Episode 85\tAverage Score: 28.16\tMax Score: 37.89\tStd Score: 13.85\n",
      "Episode 90\tAverage Score: 28.53\tMax Score: 37.89\tStd Score: 13.54\n",
      "Episode 95\tAverage Score: 28.91\tMax Score: 37.89\tStd Score: 13.28\n",
      "Episode 100\tAverage Score: 29.28\tMax Score: 37.89\tStd Score: 13.05\n",
      "\n",
      "Environment solved in 103 episodes!\tAverage Score: 30.33\n",
      "{'ACTOR_HIDDEN_LAYERS': [400, 300, 200, 100], 'CRITIC_HIDDEN_LAYERS': [400, 300, 200, 100], 'USE_BATCH_NORM': True, 'ADD_NOISE': True, 'GRAD_CLIP': False, 'BUFFER_SIZE': 100000, 'BATCH_SIZE': 128, 'GAMMA': 0.99, 'TAU': 0.001, 'LR_ACTOR': 0.0001, 'LR_CRITIC': 0.0001, 'WEIGHT_DECAY': 0, 'UPDATE_EVERY': 1, 'UPDATE_FREQ': 1, 'USE_XAVIER': False}\n",
      "Episode 5\tAverage Score: 1.01\tMax Score: 1.13\tStd Score: 0.10\n",
      "Episode 10\tAverage Score: 1.83\tMax Score: 5.19\tStd Score: 1.30\n",
      "Episode 15\tAverage Score: 6.08\tMax Score: 24.97\tStd Score: 7.37\n",
      "Episode 20\tAverage Score: 12.60\tMax Score: 35.98\tStd Score: 13.06\n",
      "Episode 25\tAverage Score: 17.57\tMax Score: 37.72\tStd Score: 15.34\n",
      "Episode 30\tAverage Score: 20.97\tMax Score: 38.47\tStd Score: 15.93\n",
      "Episode 35\tAverage Score: 23.28\tMax Score: 38.47\tStd Score: 15.81\n",
      "Episode 40\tAverage Score: 24.99\tMax Score: 38.47\tStd Score: 15.46\n",
      "Episode 45\tAverage Score: 26.15\tMax Score: 38.47\tStd Score: 14.94\n",
      "Episode 50\tAverage Score: 27.10\tMax Score: 38.47\tStd Score: 14.46\n",
      "Episode 55\tAverage Score: 27.85\tMax Score: 38.47\tStd Score: 13.99\n",
      "Episode 60\tAverage Score: 28.50\tMax Score: 38.47\tStd Score: 13.57\n",
      "Episode 65\tAverage Score: 29.02\tMax Score: 38.47\tStd Score: 13.16\n",
      "Episode 70\tAverage Score: 29.45\tMax Score: 38.47\tStd Score: 12.78\n",
      "Episode 75\tAverage Score: 29.81\tMax Score: 38.47\tStd Score: 12.42\n",
      "\n",
      "Environment solved in 78 episodes!\tAverage Score: 30.03\n",
      "{'ACTOR_HIDDEN_LAYERS': [400, 300], 'CRITIC_HIDDEN_LAYERS': [400, 300], 'USE_BATCH_NORM': True, 'ADD_NOISE': True, 'GRAD_CLIP': False, 'BUFFER_SIZE': 100000, 'BATCH_SIZE': 128, 'GAMMA': 0.99, 'TAU': 0.001, 'LR_ACTOR': 0.0001, 'LR_CRITIC': 0.001, 'WEIGHT_DECAY': 0, 'UPDATE_EVERY': 1, 'UPDATE_FREQ': 1, 'USE_XAVIER': False}\n",
      "Episode 5\tAverage Score: 0.24\tMax Score: 0.83\tStd Score: 0.31\n",
      "Episode 10\tAverage Score: 0.51\tMax Score: 0.95\tStd Score: 0.40\n",
      "Episode 15\tAverage Score: 0.59\tMax Score: 1.05\tStd Score: 0.38\n",
      "Episode 20\tAverage Score: 0.63\tMax Score: 1.05\tStd Score: 0.34\n",
      "Episode 25\tAverage Score: 0.79\tMax Score: 1.64\tStd Score: 0.44\n",
      "Episode 30\tAverage Score: 1.06\tMax Score: 3.24\tStd Score: 0.75\n",
      "Episode 35\tAverage Score: 1.67\tMax Score: 6.47\tStd Score: 1.69\n",
      "Episode 40\tAverage Score: 2.36\tMax Score: 8.73\tStd Score: 2.45\n",
      "Episode 45\tAverage Score: 2.97\tMax Score: 8.73\tStd Score: 2.89\n",
      "Episode 50\tAverage Score: 3.80\tMax Score: 13.12\tStd Score: 3.72\n",
      "Episode 55\tAverage Score: 4.92\tMax Score: 20.29\tStd Score: 5.06\n",
      "Episode 60\tAverage Score: 6.19\tMax Score: 22.81\tStd Score: 6.44\n",
      "Episode 65\tAverage Score: 7.63\tMax Score: 27.64\tStd Score: 7.98\n",
      "Episode 70\tAverage Score: 9.31\tMax Score: 33.72\tStd Score: 9.80\n",
      "Episode 75\tAverage Score: 11.03\tMax Score: 37.88\tStd Score: 11.48\n",
      "Episode 80\tAverage Score: 12.61\tMax Score: 37.88\tStd Score: 12.68\n",
      "Episode 85\tAverage Score: 13.99\tMax Score: 37.88\tStd Score: 13.49\n",
      "Episode 90\tAverage Score: 15.07\tMax Score: 37.88\tStd Score: 13.85\n",
      "Episode 95\tAverage Score: 16.10\tMax Score: 37.88\tStd Score: 14.17\n",
      "Episode 100\tAverage Score: 16.89\tMax Score: 37.88\tStd Score: 14.24\n",
      "Episode 105\tAverage Score: 18.52\tMax Score: 37.88\tStd Score: 14.11\n",
      "Episode 110\tAverage Score: 20.21\tMax Score: 37.88\tStd Score: 13.91\n",
      "Episode 115\tAverage Score: 21.85\tMax Score: 37.88\tStd Score: 13.44\n",
      "Episode 120\tAverage Score: 23.52\tMax Score: 37.88\tStd Score: 12.79\n",
      "Episode 125\tAverage Score: 25.15\tMax Score: 37.88\tStd Score: 11.92\n",
      "Episode 130\tAverage Score: 26.75\tMax Score: 37.88\tStd Score: 10.87\n",
      "Episode 135\tAverage Score: 28.13\tMax Score: 37.88\tStd Score: 9.75\n",
      "Episode 140\tAverage Score: 29.34\tMax Score: 37.88\tStd Score: 8.51\n",
      "\n",
      "Environment solved in 143 episodes!\tAverage Score: 30.10\n",
      "{'ACTOR_HIDDEN_LAYERS': [400, 300, 200, 100], 'CRITIC_HIDDEN_LAYERS': [400, 300, 200, 100], 'USE_BATCH_NORM': True, 'ADD_NOISE': True, 'GRAD_CLIP': False, 'BUFFER_SIZE': 100000, 'BATCH_SIZE': 128, 'GAMMA': 0.99, 'TAU': 0.001, 'LR_ACTOR': 0.0001, 'LR_CRITIC': 0.001, 'WEIGHT_DECAY': 0, 'UPDATE_EVERY': 1, 'UPDATE_FREQ': 1, 'USE_XAVIER': False}\n",
      "Episode 5\tAverage Score: 0.16\tMax Score: 0.26\tStd Score: 0.07\n",
      "Episode 10\tAverage Score: 0.11\tMax Score: 0.26\tStd Score: 0.09\n",
      "Episode 15\tAverage Score: 0.08\tMax Score: 0.26\tStd Score: 0.08\n",
      "Episode 20\tAverage Score: 0.11\tMax Score: 0.32\tStd Score: 0.11\n",
      "Episode 25\tAverage Score: 0.22\tMax Score: 1.34\tStd Score: 0.30\n",
      "Episode 30\tAverage Score: 0.59\tMax Score: 3.17\tStd Score: 0.93\n",
      "Episode 35\tAverage Score: 1.37\tMax Score: 8.82\tStd Score: 2.21\n",
      "Episode 40\tAverage Score: 3.22\tMax Score: 19.30\tStd Score: 5.36\n",
      "Episode 45\tAverage Score: 5.52\tMax Score: 29.92\tStd Score: 8.42\n",
      "Episode 50\tAverage Score: 8.55\tMax Score: 37.61\tStd Score: 12.10\n",
      "Episode 55\tAverage Score: 11.11\tMax Score: 37.61\tStd Score: 14.10\n",
      "Episode 60\tAverage Score: 13.17\tMax Score: 37.61\tStd Score: 15.13\n",
      "Episode 65\tAverage Score: 14.87\tMax Score: 37.61\tStd Score: 15.68\n",
      "Episode 70\tAverage Score: 16.33\tMax Score: 37.61\tStd Score: 16.01\n",
      "Episode 75\tAverage Score: 17.65\tMax Score: 37.61\tStd Score: 16.24\n",
      "Episode 80\tAverage Score: 18.70\tMax Score: 37.61\tStd Score: 16.24\n",
      "Episode 85\tAverage Score: 19.41\tMax Score: 37.61\tStd Score: 16.11\n"
     ]
    }
   ],
   "source": [
    "experiments = [agent_params, agent_params1, agent_params2, agent_params3]\n",
    "exp_count=1\n",
    "for exp in experiments:\n",
    "    print(exp)\n",
    "    folder = \"./experiments/\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    with open(folder  +'/exp_' + str(exp_count) + '.json', 'w') as fp:\n",
    "        json.dump(exp, fp)\n",
    "    agent = Agent(state_size, action_size, num_agents, 42, exp)\n",
    "    scores = ddpg_train(agent, env, brain_name, num_agents,\n",
    "                         actor_model_pth= folder + 'actor_model_exp_'+str(exp_count) +'.pth',\n",
    "                        critic_model_pth=folder + 'critic_model_exp_'+str(exp_count) +'.pth',\n",
    "\t\t\t\t\t        n_episodes=1500,  print_every=5)\n",
    "    with open(folder +'/exp_' + str(exp_count) + '_scores.pkl', 'wb') as fp:\n",
    "        pickle.dump(scores, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    plot_experiment(scores, exp_count)\n",
    "    exp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparison of the executed experiments\n",
    "\n",
    "Below you can see the effect of the previous experiments on the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./experiments/\"\n",
    "plt.figure(figsize=(15, 8))\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        with open(folder + file, 'rb') as fp:\n",
    "            scores = pickle.load(fp)\n",
    "            plt.plot(np.arange(1, len(scores) + 1), np.squeeze(np.vstack(scores)), label=str(file))\n",
    "plt.axis('equal');\n",
    "plt.title('Comparison of the experiments')\n",
    "plt.legend();\n",
    "plt.savefig(\"./experiments/comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Present a previously trained agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 1 # set id of experiment to present...\n",
    "folder = \"./experiments/\"\n",
    "agent = Agent(state_size, action_size, num_agents, 42, agent_params)\n",
    "ddpg_present(agent, env, brain_name, num_agents,\n",
    "                         actor_model_pth= folder + \"solution_actor_model_exp_\" + exp_id + \".pth\",\n",
    "                        critic_model_pth=folder + \"solution_critic_model_exp_\" + exp_id + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
