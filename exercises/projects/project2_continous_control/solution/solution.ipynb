{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program. We also demonstrate a possible solution for the second project.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).\n",
    "\n",
    "We will use here the headless version since the project is set up in a dockerized environment to execute it on remote computing (possible GPU enabled)\n",
    "resources as well. \n",
    "\n",
    "\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "from ddpg_utils import ddpg_train, ddpg_present\n",
    "\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='../reacher2/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment note: train_mode is True since we would like to train our agent here\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the agent with DDPG\n",
    "\n",
    "During the assigment we train the agent with `Deep Deterministic Policy Gradient` algorithm. You can find an outstanding description about it \n",
    "[here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html). The underlying deep(? - maybe rather wide than deep) neural networks for the actor and the critic are defined in [model.py](./model.py). The agent itself in the [ddpg_agent.py](./ddpg_agent.py). These implementations are based on the examples provided for the course for the actor-critic methods chapter [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal) or [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum). \n",
    "\n",
    "I choose to solve the First Version as: the task is episodic, and in order to solve the environment, the agent must get an average score of +30 over 100 consecutive episodes.\n",
    "\n",
    "Next we define a function to execute the training itself, which we will use for later experiments with the hyperparams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params = {\n",
    "    'ACTOR_HIDDEN_LAYERS': [400, 300],\n",
    "    'CRITIC_HIDDEN_LAYERS': [400, 300],\n",
    "    'USE_BATCH_NORM': True,\n",
    "    'ADD_NOISE': True,\n",
    "    'GRAD_CLIP': False,\n",
    "    'BUFFER_SIZE': int(1e5),\n",
    "    'BATCH_SIZE': 128,\n",
    "    'GAMMA': 0.99,\n",
    "    'TAU': 1e-3,\n",
    "    'LR_ACTOR': 1e-4,\n",
    "    'LR_CRITIC': 1e-4,\n",
    "    'WEIGHT_DECAY': 0,#1e-2,\n",
    "    'UPDATE_EVERY': 1,\n",
    "    'UPDATE_FREQ': 1,\n",
    "    'USE_XAVIER': False,\n",
    "}\n",
    "agent_params1 = {**agent_params,\n",
    "                 'ACTOR_HIDDEN_LAYERS': [400, 300, 200, 100],\n",
    "                 'CRITIC_HIDDEN_LAYERS': [400, 300, 200, 100],\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACTOR_HIDDEN_LAYERS': [400, 300], 'CRITIC_HIDDEN_LAYERS': [400, 300], 'USE_BATCH_NORM': True, 'ADD_NOISE': True, 'GRAD_CLIP': False, 'BUFFER_SIZE': 100000, 'BATCH_SIZE': 128, 'GAMMA': 0.99, 'TAU': 0.001, 'LR_ACTOR': 0.0001, 'LR_CRITIC': 0.0001, 'WEIGHT_DECAY': 0, 'UPDATE_EVERY': 1, 'UPDATE_FREQ': 1, 'USE_XAVIER': False}\n",
      "Episode 5\tAverage Score: 0.90\tMax Score: 1.22\tStd Score: 0.22\n"
     ]
    }
   ],
   "source": [
    "experiments = [agent_params, agent_params1]\n",
    "exp_count=1\n",
    "for exp in experiments:\n",
    "    print(exp)\n",
    "    folder = \"./experiments/\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    with open(folder  +'/exp_' + str(exp_count) + '.json', 'w') as fp:\n",
    "        json.dump(exp, fp)\n",
    "    agent = Agent(state_size, action_size, num_agents, 42, exp)\n",
    "    scores = ddpg_train(agent, env, brain_name, num_agents,\n",
    "                         actor_model_pth= folder + 'actor_model_exp_'+str(exp_count) +'.pth',\n",
    "                        critic_model_pth=folder + 'critic_model_exp_'+str(exp_count) +'.pth',\n",
    "\t\t\t\t\t        n_episodes=1500,  print_every=5)\n",
    "    with open(folder +'/exp_' + str(exp_count) + '_scores.pkl', 'wb') as fp:\n",
    "        pickle.dump(scores, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    exp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
